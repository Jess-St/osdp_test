{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Generation Map: BRMS API Calls and Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from ElexonDataPortal import api\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import py_versions.pipeline_fns as plfns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jessica.steinemann\\\\OneDrive - Energy Systems Catapult Ltd\\\\projects\\\\osdp_test'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osdp_folder = os.environ.get(\"OSDP\")\n",
    "osdp_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which folder contains your local directory\n",
    "(\n",
    "    location,\n",
    "    location_BMRS,\n",
    "    location_BMRS_PHYBMDATA,\n",
    "    location_BMRS_B1610,\n",
    "    location_BMRS_Final,\n",
    ") = plfns.create_folder_structure(osdp_folder=osdp_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Diff Querying / Change Data Capture (CDC)\n",
    "For both the B1610 data and the PHYBMDATA we want to check if these datasets already exist in the \"OSDP\" directory, and create them if not. <br> <br>\n",
    "We then want to check for the difference since the pipeline was last run as this will be more efficient than requesting all the historic and BM data each time the pipeline is run. <br><br>\n",
    "In order to achieve this, the script first checks for updates to the historic generation by BMU (B1610 report), i.e. whether new B1610 data is available since the pipeline was last run. NB, this report only updates once daily for one entire day. If the B1610 data hasn't been updated for longer than the \"num_days\" variable, then the function will automatically cause the  \"get_setup_B1610_data\" function to run to create a new dataset. <br><br>\n",
    "Once the B1610 data has been updated, the script then checks for updates to the Balancing Mechanism Physical data: it removes any data that has now been replaced with historic data, then proceeds to query for new physical data. The period queried will be the first period since the physical data was last queried until the end of the current day. NB, physical data is updated half-hourly. Hence, this script should eventually run every 30 min.<br><br>\n",
    "At the end, the updated dataframes overwrite the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1610: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "df_B1610 = plfns.setup_update_B1610_data(location_BMRS_B1610=location_BMRS_B1610, num_days=14, hist_days=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_start_date = pd.to_datetime(df_B1610[\"settlementDate\"].max() + timedelta(days=1)).replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessica.steinemann\\AppData\\Local\\Continuum\\Anaconda3\\envs\\OSDP\\Lib\\site-packages\\ElexonDataPortal\\dev\\utils.py:119: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  df_dates_SPs = df_dates_SPs[start_date:end_date]\n",
      "PHYBMDATA: 100%|██████████| 7/7 [00:30<00:00,  4.32s/it]\n",
      "c:\\Users\\jessica.steinemann\\AppData\\Local\\Continuum\\Anaconda3\\envs\\OSDP\\Lib\\site-packages\\ElexonDataPortal\\dev\\utils.py:119: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  df_dates_SPs = df_dates_SPs[start_date:end_date]\n"
     ]
    }
   ],
   "source": [
    "df_PHYBMDATA = plfns.setup_update_PHYBM_data(\n",
    "    BM_start_date=BM_start_date, location_BMRS_PHYBMDATA=location_BMRS_PHYBMDATA\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the BM data to follow a similar patterns as the historic data\n",
    "Next, the balancing mechanism data should be filtered and transformed so that it follows a similar pattern as the B1610 data. <br> <br>\n",
    "Abbreviations (https://www.bmreports.com/bmrs/?q=help/glossary): <br>\n",
    "* **FPN**: Final Physical Notification - \"A Physical Notification is the best estimate of the level of generation or demand that a participant in the BM expects a BM Unit to export or import, respectively, in a Settlement Period.\"\n",
    "* **BOAL(F)**: Bid Offer Acceptance Level - subsequent \"last minute\" changes to this notified generation, e.g. due to curtailment or due to balancing demands. \"A Bid-Offer Acceptance is a formalised representation of the purchase and/or sale of Offers and/or Bids (see Bid-Offer Data below) by the System Operator in its operation of the Balancing Mechanism.\"\n",
    "* **MEL**: Maximum Export Level - It is the maximum power export level of a particular BM Unit at a particular time. It is submitted as a series of point MW values and associated times. <br><br>\n",
    "The actions to turn the BM data into long format and resolve it to minutely level will only be performed on the latest BM data to reduce the processing time and compute required. The rest of the BM data will be read from the previous version of the output dataset, i.e. anything between the BM_Start_date and 90min from the end date of the previous version of the \"Generation_Combined.csv\" output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generation, df_fpn, df_mel, df_boal = plfns.filter_and_rename_physical_Data(\n",
    "    location_BMRS_Final, df_B1610, df_PHYBMDATA\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The half-hourly or sub-half-hourly data is resampled to minutely resolution so that actions that happen at different times during each half-hour period can be joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boal_long = plfns.convert_physical_data_to_long(df_boal)\n",
    "unit_boal_resolved = plfns.resolve_applied_bid_offer_level(df_boal_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessica.steinemann\\OneDrive - Energy Systems Catapult Ltd\\projects\\osdp_test\\notebooks\\py_versions\\pipeline_fns.py:333: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_index, data in df_linear.groupby(groupby):\n"
     ]
    }
   ],
   "source": [
    "df_fpn_long = plfns.convert_physical_data_to_long(df_fpn)\n",
    "unit_fpn_resolved = plfns.resolve_FPN_MEL_level(df_fpn_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessica.steinemann\\OneDrive - Energy Systems Catapult Ltd\\projects\\osdp_test\\notebooks\\py_versions\\pipeline_fns.py:333: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_index, data in df_linear.groupby(groupby):\n"
     ]
    }
   ],
   "source": [
    "df_mel_long = plfns.convert_physical_data_to_long(df_mel)\n",
    "unit_mel_resolved = plfns.resolve_FPN_MEL_level(df_mel_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After resampling the data to minutely resolution (Time), join the FPN, BOAL and MEL data.\n",
    "\n",
    "df_fpn_boal = pd.merge(\n",
    "    unit_fpn_resolved, unit_boal_resolved, how=\"outer\", on=[\"Time\", \"bmUnitID\"], suffixes=[\"_fpn\", \"_boal\"]\n",
    ")\n",
    "\n",
    "df_fpn_mel_boal = pd.merge(df_fpn_boal, unit_mel_resolved, how=\"outer\", on=[\"Time\", \"bmUnitID\"]).rename(\n",
    "    columns={\"Level\": \"Level_mel\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fpn_mel_boal[\"quantity\"] = df_fpn_mel_boal[\"Level_boal\"].fillna(\n",
    "    df_fpn_mel_boal[\"Level_fpn\"], inplace=False\n",
    ")  # If a BOAL value exists, use it. Otherwise, retain the FPN value (which will always exist).\n",
    "df_fpn_mel_boal[\"quantity\"] = np.where(\n",
    "    df_fpn_mel_boal[\"quantity\"] > df_fpn_mel_boal[\"Level_mel\"],\n",
    "    df_fpn_mel_boal[\"Level_mel\"],\n",
    "    df_fpn_mel_boal[\"quantity\"],\n",
    ")  # If the MEL is lower than the BOAL or FPN value, cap the generation at the level of the MEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the curtailment amount\n",
    "df_fpn_mel_boal[\"quantity_fpn_diff\"] = np.where(\n",
    "    (df_fpn_mel_boal[\"Level_boal\"] >= 0) & (df_fpn_mel_boal[\"Level_mel\"] >= df_fpn_mel_boal[\"Level_boal\"]),\n",
    "    df_fpn_mel_boal[\"Level_fpn\"] - df_fpn_mel_boal[\"Level_boal\"],\n",
    "    np.NaN,\n",
    ")\n",
    "df_fpn_mel_boal[\"quantity_curtailment\"] = np.where(\n",
    "    df_fpn_mel_boal[\"quantity_fpn_diff\"] >= 0, df_fpn_mel_boal[\"quantity_fpn_diff\"], 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.steinemann\\AppData\\Local\\Temp\\ipykernel_23704\\1663645550.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  .mean()\n"
     ]
    }
   ],
   "source": [
    "# Aggregate back up to the settlement period (SP) level and calculate the mean generation during each SP\n",
    "df_fpn_mel_boal[\"settlementPeriod_fpn\"] = df_fpn_mel_boal[\"settlementPeriod_fpn\"].astype(str)\n",
    "df_fpn_mel_boal_agg = (\n",
    "    df_fpn_mel_boal.groupby([\"local_datetime_fpn\", \"settlementDate\", \"settlementPeriod\", \"bmUnitID\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "df_fpn_mel_boal_agg = df_fpn_mel_boal_agg.rename(columns={\"local_datetime_fpn\": \"local_datetime\"})\n",
    "df_fpn_mel_boal_agg = df_fpn_mel_boal_agg[\n",
    "    [\n",
    "        \"local_datetime\",\n",
    "        \"settlementDate\",\n",
    "        \"settlementPeriod\",\n",
    "        \"bmUnitID\",\n",
    "        \"quantity\",\n",
    "        \"quantity_curtailment\",\n",
    "        \"quantity_fpn_diff\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B1610[\"quantity\"] = df_B1610[\"quantity\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generation = pd.concat((df_B1610, df_generation, df_fpn_mel_boal_agg), axis=0)\n",
    "df_generation = df_generation[\n",
    "    df_generation[\"quantity\"] > 0\n",
    "].copy()  # Filter out BM data with a negative value (not a generator) or a value of 0 (B1610 only has positive values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the BMRS data with the Power Station Dictionary Names and Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd_merged = pd.read_csv(os.path.join(location, \"merged_psd.csv\"), header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.steinemann\\AppData\\Local\\Temp\\ipykernel_23704\\174233568.py:1: FutureWarning: Passing 'suffixes' which cause duplicate columns {'fuel_x', 'latitude_x', 'longitude_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_generation = df_generation.merge(df_psd_merged, how=\"left\", left_on=\"bmUnitID\", right_on=\"sett_bmuID\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['longitude', 'latitude', 'fuel'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_generation \u001b[39m=\u001b[39m df_generation\u001b[39m.\u001b[39mmerge(df_psd_merged, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, left_on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbmUnitID\u001b[39m\u001b[39m\"\u001b[39m, right_on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msett_bmuID\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df_generation \u001b[39m=\u001b[39m df_generation[\n\u001b[0;32m      3\u001b[0m     [\n\u001b[0;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlocal_datetime\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msettlementDate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msettlementPeriod\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbmUnitID\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquantity\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquantity_curtailment\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquantity_fpn_diff\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mdictionary_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcommon_name\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mfuel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     16\u001b[0m     ]\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     18\u001b[0m df_generation \u001b[39m=\u001b[39m df_generation\u001b[39m.\u001b[39mrename(\n\u001b[0;32m     19\u001b[0m     columns\u001b[39m=\u001b[39m{\n\u001b[0;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlocal_datetime\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlocalDateTime\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     }\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jessica.steinemann\\AppData\\Local\\Continuum\\Anaconda3\\envs\\OSDP\\Lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jessica.steinemann\\AppData\\Local\\Continuum\\Anaconda3\\envs\\OSDP\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jessica.steinemann\\AppData\\Local\\Continuum\\Anaconda3\\envs\\OSDP\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['longitude', 'latitude', 'fuel'] not in index\""
     ]
    }
   ],
   "source": [
    "df_generation = df_generation.merge(df_psd_merged, how=\"left\", left_on=\"bmUnitID\", right_on=\"sett_bmuID\")\n",
    "df_generation = df_generation[\n",
    "    [\n",
    "        \"local_datetime\",\n",
    "        \"settlementDate\",\n",
    "        \"settlementPeriod\",\n",
    "        \"bmUnitID\",\n",
    "        \"quantity\",\n",
    "        \"quantity_curtailment\",\n",
    "        \"quantity_fpn_diff\",\n",
    "        \"dictionary_id\",\n",
    "        \"common_name\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"fuel\",\n",
    "    ]\n",
    "]\n",
    "df_generation = df_generation.rename(\n",
    "    columns={\n",
    "        \"local_datetime\": \"localDateTime\",\n",
    "        \"bmUnitID\": \"BMUnitID\",\n",
    "        \"dictionary_id\": \"dictionaryID\",\n",
    "        \"common_name\": \"commonName\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default values for dashboard\n",
    "df_generation[\"dictionaryID\"] = np.where(df_generation[\"dictionaryID\"].isnull(), 99999, df_generation[\"dictionaryID\"])\n",
    "df_generation[\"commonName\"] = np.where(\n",
    "    df_generation[\"commonName\"].isnull(), \"Unknown Name/Location\", df_generation[\"commonName\"]\n",
    ")\n",
    "df_generation[\"longitude\"] = np.where(df_generation[\"longitude\"].isnull(), -2.547855, df_generation[\"longitude\"])\n",
    "df_generation[\"latitude\"] = np.where(df_generation[\"latitude\"].isnull(), 54.00366, df_generation[\"latitude\"])\n",
    "df_generation[\"fuel\"] = np.where(df_generation[\"fuel\"].isnull(), \"Unknown Fuel\", df_generation[\"fuel\"])\n",
    "\n",
    "\n",
    "# Split data into renewable/non-renewable\n",
    "df_generation[\"lowCarbonGeneration\"] = np.where(\n",
    "    df_generation[\"fuel\"].isin([\"BIOMASS\", \"NPSHYD\", \"NUCLEAR\", \"PS\", \"WIND\", \"Wind\"]),\n",
    "    \"Low Carbon Generation\",\n",
    "    \"Carbon Intensive Generation\",\n",
    ")\n",
    "df_generation[\"renewableGeneration\"] = np.where(\n",
    "    df_generation[\"fuel\"].isin([\"BIOMASS\", \"NPSHYD\", \"PS\", \"WIND\", \"Wind\"]),\n",
    "    \"Renewable Generation\",\n",
    "    \"Non-Renewable Generation\",\n",
    ")\n",
    "\n",
    "# Give the Fuel Types a more friendly name\n",
    "fuel_type_friendly = {\n",
    "    \"BIOMASS\": \"Biomass\",\n",
    "    \"CCGT\": \"Combined-cycle Gas Turbine\",\n",
    "    \"COAL\": \"Coal\",\n",
    "    \"OCGT\": \"Open-cycle Gas Turbine\",\n",
    "    \"NPSHYD\": \"Other Hydro\",\n",
    "    \"NUCLEAR\": \"Nuclear\",\n",
    "    \"PS\": \"Pumped Storage Hydro\",\n",
    "    \"WIND\": \"Wind\",\n",
    "    \"Wind\": \"Wind\",\n",
    "}\n",
    "\n",
    "df_generation[\"fuel\"] = df_generation[\"fuel\"].replace(to_replace=fuel_type_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generation.to_csv(os.path.join(location_BMRS_Final, \"Generation_Combined.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os_data_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "330e47966bd248890b7f6cb1b4c1f2af56f7d2fe136ab68068e8a59c999dca59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
